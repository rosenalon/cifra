var abstracts = new Map();

function init(){
	abstracts.set("marc_m","This is a informal presentation and discussion, opened to all those who are interested, about the possibility of creating a new scientific Journal on the theory of AI and Machine Learning. The idea would be to have a regular journal dedicated to all aspects of the theory, including papers of relevant subfields of math, computer science, statistics, physics etc. The idea came from internal discussions within Sissa Medialab, the publisher of successful journals of theoretical physics, including the Journal of Statistical Mechanics Theory of Experiment, and the Journal of High Energy Physics. These are journals which want to be at the service of the community, run by the community, and this is the type of idea that we are exploring. The CEO of Sissa Medialab, Aldo Rampioni, will also be present. The idea of the discussion is to exchange ideas on these issues");
	abstracts.set("michael","Nonlocal games are a foundational tool in quantum information and complexity. They give an operational perspective on entanglement, which in turn has led to many protocols in settings with multiple spatially-separated quantum devices. A recent line of work initiated by Kalai et al (STOC'23) investigates to which extent spatial separation can be replaced by timelike separation, by using cryptography. I will give a gentle introduction to this topic and its motivations and discuss a recent result that characterizes the well-known class of \"XOR games\" in this setting. This resolves an open question and establishes a computational version of Tsirelson's theorem.");
	abstracts.set("vincent","The scale of modern machine learning models and data has made data selection a central problem. In this talk, we focus on the problem of finding the best representative subset of a dataset to train a machine learning model. We provide a new data selection approach based on \\(k\\)-means clustering and sensitivity sampling.<br><br> Assuming embedding representation of the data and that the model loss is Hölder continuous with respect to these embeddings, we prove that our new approach allows to select a set of ''typical'' \\(1/\\varepsilon^2\\) elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative \\((1\\pm\\varepsilon)\\) factor and an additive \\(\\varepsilon\\lambda\\Phi_k\\), where \\(\\Phi_k\\) represents the \\(k\\)-means cost for the input data and \\(\\lambda\\) is the Hölder constant. We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods.<br><br>We also show that our sampling strategy can be used to define new sampling scores for regression, leading to a new active learning strategy that is comparatively simpler and faster than previous ones like leverage score.<br><br>To appear at ICML'24, joint work with Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder.");
	abstracts.set("giuseppe","In this presentation I will discuss the problem of simulating the behavior of interacting quantum matter, from first principles, and how machine-learning inspired techniques are shifting the state of the art in the field. <br><br>After introducing the notion of Neural Quantum States, I will show applications to both equilibrium and out of equilibrium properties of electrons, with applications ranging from Condensed Matter to Quantum Chemistry. ");
    abstracts.set("keisuke","Quantum machine learning, promising for quantum computers, explores implicit models utilizing quantum kernel methods or explicit models, known as quantum circuit learning. While implicit models often yield lower training errors, they face linear prediction time scaling with data size, potentially overfitting. Explicit models predict in constant time but encounter challenges with optimization, notably the barren plateau phenomenon. This study introduces a quantum-classical hybrid algorithm to convert implicit models efficiently to explicit ones. The resulting explicit model matches implicit model performance but requires fewer quantum circuit executions for inference. In classification tasks using MNISQ and VQE-generated datasets, our explicit model shows comparable generalization to implicit models with reduced computational costs. Our algorithm accelerates prediction for implicit models and aids in constructing high-performance explicit models, notably addressing the barren plateau phenomenon. We also discuss our efforts in developing infrastructure for quantum machine learning, including datasets, libraries, and simulators.");
	abstracts.set("giulio_b","We present a study of generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. We reveals the existence of three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For realistic dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.");
	abstracts.set("giulio_m","We propose a method to analyze the query complexity of quantum algorithms with oracle access to a random permutation and its inverse. Our work is motivated by applications in cryptography. In particular, we aim at analyzing the quantum security of hash functions, such as the sponge construction, which is routinely used in real-world cryptographic protocols. As an application of our framework, we show that the one-round sponge construction is unconditionally preimage resistant, in the random permutation model. This proves a conjecture by Unruh.");
	abstracts.set("song","Textbooks on deep learning theory primarily perceive neural networks as universal function approximators. While this classical viewpoint is fundamental, it inadequately explains the impressive capabilities of modern generative AI models such as language models and diffusion models. This talk puts forth a refined perspective: neural networks often serve as algorithm approximators, going beyond mere function approximation. I will explain how this refined perspective offers a deeper insight into the success of modern generative AI models.");
	abstracts.set("zohar","Gaussian Processes (GPs) provide a stepping stone for analyzing deep neural networks. Despite their humble appearances, drawing analytical conclusions from GPs can be challenging. The first difficulty arises due to dataset-averaging which generates interaction terms between GP modes. The other challenge is to diagonalize the GP kernel with respect to an unknown data distribution. In this talk, I'll discuss several recent advancements addressing these issues. Specifically, I'll show how the renormalization group (RG) allows us to solve the interacting theory obtained after dataset averaging, thereby providing a concrete example where this fundamental tool from physics leads to quantitative predictions in deep learning. Turning to GPs induced by deep non-linear Transformers, I'll discuss how permutation symmetry allows us to lower bound the amount of data needed to learn various relevant target functions such as induction heads and coping heads on real-world datasets. Central to this is a novel GP learnability bound which is largely data-agnostic. ");
	abstracts.set("johanni","");
	abstracts.set("sueyeon","Recent breakthroughs in experimental neuroscience and machine learning have opened new frontiers in understanding the computational principles governing neural circuits and artificial neural networks (ANNs). Both biological and artificial systems exhibit an astonishing degree of orchestrated information processing capabilities across multiple scales - from the microscopic responses of individual neurons to the emergent macroscopic phenomena of cognition and task functions. At the mesoscopic scale, the structures of neuron population activities manifest themselves as neural representations. Neural computation can be viewed as a series of transformations of these representations through various processing stages of the brain. The primary focus of my lab's research is to develop theories of neural representations that describe the principles of neural coding and, importantly, capture the complex structure of real data from both biological and artificial systems.<br><br>In this talk, I will present three related approaches that leverage techniques from statistical physics, machine learning, and geometry to study the multi-scale nature of neural computation. First, I will introduce new statistical mechanical theories that connect geometric structures that arise from neural responses (i.e., neural manifolds) to the efficiency of neural representations in implementing a task. Second, I will employ these theories to analyze how these representations evolve across scales, shaped by the properties of single neurons and the transformations across distinct brain regions. Finally, I will demonstrate how insights from the theories of neural representations can elucidate why certain ANN models better predict neural data, facilitating model comparison and selection.");
	abstracts.set("cengiz","On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate \\(1/\\text{width}\\) but at late time exhibit a rate \\(\\text{width}−c\\), where \\(c\\) depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.");
	abstracts.set("ran","We explore the possibility of obtaining general-purpose obfuscation for all circuits by way of making only simple, local, functionality preserving random perturbations in the circuit structure. Towards this goal, we use the additional structure provided by reversible circuits,  but no additional algebraic structure.<br><br>We start by formulating a new (and relatively weak) obfuscation task regarding the ability to obfuscate  random circuits of  bounded length.  We call such obfuscators  random input & output (RIO) obfuscators. We then show how to  construct indistinguishability obfuscators for all (unbounded length) circuits given only an RIO obfuscator ---  under a new  assumption regarding the pseudorandomness of sufficiently long random  reversible circuits with known functionality, which in turn builds on  a conjecture made by Gowers (Comb. Prob. Comp.  '96) regarding the pseudorandomness of bounded-size random reversible circuits. Furthermore, the constructed obfuscators satisfy a new measure of security - called random output indistinguishability (ROI) obfuscation - which is significantly stronger than IO  and  may be of independent interest.<br><br>We then investigate the possibility  of constructing RIO obfuscators using local,  functionality preserving  perturbations. Our approach  is rooted in statistical mechanics and can be thought of as locally ``thermalizing''  a circuit while preserving its functionality. We provide  candidate constructions along with a pathway for analyzing the security of such strategies. <br><br>Given the power of program obfuscation, viability of the proposed approach would provide an alternative route to realizing almost all cryptographic tasks under hardness assumptions that are  very different from standard ones. Furthermore,  our specific candidate obfuscators are relatively efficient: the obfuscated version of an \\(n\\)-wire, \\(m\\)-gate (reversible) circuit with security parameter \\(k\\)  has \\(n\\) wires and \\(\\text{poly}(n,k)\\,m\\)  gates.   We hope that our initial exploration will motivate further study of this alternative path to cryptography.");
	abstracts.set("guilhem","In this talk I will discuss the additive version of the matrix denoising problem, where a random symmetric matrix \\(S\\) has to be inferred from the observation of \\(Y=S+Z\\), with \\(Z\\) an independent random matrix modeling a noise. Systematic approximations to the Bayes-optimal estimator of \\(S\\) can be built by considering polynomial estimators. When the prior distributions on \\(S\\) and \\(Z\\) are orthogonally invariant this procedure allows to recover asymptotically the estimator introduced by Bun, Allez, Bouchaud and Potters in 2016. It also opens the way to the discussion of finite-size corrections, and to non-orthogonally invariant priors. A special case of particular interest occurs when \\(S\\) has a Wishart distribution, the denoising problem being then a simplified version of the extensive rank matrix factorization problem. ");
	abstracts.set("yue","");
	abstracts.set("matteo","");
	abstracts.set("stephane","Diffusions by denoising score matching generate impressive images. Are they generalising or memorising ? How precise is the score estimation ? What is the relation with previous denoising algorithms ? How can deep network estimations of scores avoid the curse of dimensionality ? How does it relate to physics ?  I will try to partly address these questions.");
	abstracts.set("florent","");
	abstracts.set("matthieu","Learning generic tasks in high dimension is impossible, as it would require an unreachable number of training data. Yet algorithms or humans can play the game of go, decide what is on an image or learn languages. The only resolution of this paradox is that learnable data are highly structured. I will review ideas in the field of what this structure may be. I will then discuss two recent results of our group. (i) if the data  is hierarchical, supervised learning can occur with a training set size which is polynomial, and not exponential, in the data dimension. (ii) I will discuss how novel data can be generated by composing known features into a new whole. This theory of composition predicts a phase transition  in score-based diffusion generative models. I will discuss empirical evidence for its validity.");
	abstracts.set("marc","We explore the power of graph neural networks (GNNs) for solving a combinatorial optimization problem: the graph matching problem, which aims to match unlabeled graphs using only topological information. In a supervised setting, we train GNNs to enhance a partial solution so that during inference, the GNNs are chained to bootstrap the performance. We show that our chaining procedure can be coupled with an existing solver based on the Frank-Wolf algorithm as a last step to allow us to recover the optimal solution in cases where no other algorithm is able to do so.");
	abstracts.set("federico","");
	abstracts.set("moritz","A key property of neural networks driving their success is their ability to learn features from data. We here study learning in the proportional limit where the number of data points \\(P= \\alpha N \\to \\infty\\) and describe learning as kernel adaptation.  A large deviation approach, which is exact in this limit, yields a pair of forward-backward equations for the maximum a posteriori kernels in all layers at once. We study their solutions perturbatively to demonstrate how the backward propagation across layers aligns kernels with the target.  We then show that this pair of equations captures the finite N fluctuation corrections to the NNGP limit. Larger fluctuations here correspond to a more flexible network prior and thus enable stronger adaptation to data. These are most pronounced at a critical point controlled by the hyperparameters. We thus find a bridge between the classical edge-of-chaos NNGP theory and feature learning, exposing an intricate interplay between criticality and feature scale.  We finally point out the general applicability of the approach by showing that deep fully connected, recurrent networks, and resnets may be treated in the same formalism.");
	abstracts.set("pravesh","In this talk, we will present a new approach for approximating large independent sets when the input graph is a one-sided expander—that is, the uniform random walk matrix of the graph has a second eigenvalue bounded away from 1. Specifically, we show a polynomial time algorithm that can find linear size-independent sets in one-sided expanders that are almost \\(3\\)-colorable or promised to contain a \\(0.4999n\\) size-independent set.<br><br>All prior algorithms that beat the worst-case guarantees for the problem rely on bottom eigenspace enumeration techniques building Alon and Kahale's classical work and need two-sided expansion (i.e., bounded number of negative eigenvalues of magnitude close to 1). Such techniques must provably fail in our setting because in contrast to bottom eigenspace enumeration that naturally extends to \\(k\\)-colorable graphs for any fixed constant \\(k\\).  We observe that finding linear size independent sets in almost \\(4\\)-colorable graphs (as opposed to almost \\(3\\)-colorable graphs in our algorithmic result above) is NP-hard, assuming the Unique Games Conjecture.<br><br>Our algorithms rely on a new framework for rounding sum-of-squares relaxations of the independent set problem based on a new clustering property of large independent sets in expanders —in any graph that satisfies one-sided spectral expansion (in fact, a weaker vertex expansion property suffices), there is a small list of independent sets such that every large independent sets correlates better than a random set of similar size with at least one member of the list. This is reminiscent of similar clustering properties of solutions space of random optimization problems. Our applications, however, are on deterministic expander graphs, and our algorithmic guarantees need a low-degree sum-of-squares formalization of the clustering property.<br><br>Based on joint work with Mitali Bafna and Tim Hsieh.");
	abstracts.set("rajendra","");
	abstracts.set("shuichi","We show the converse of the celebrated theorem of Goldreich, Micali, and Wigderson (J. ACM 1991), thereby characterizing the existence of a one-way function by the worst-case complexities of zero knowledge.  Specifically, the following are equivalent: <ol><li>A one-way function exists.</li><li>\\(\\mathrm{NP}\\subseteq\\mathrm{CZK}\\) and \\(\\mathrm{NP}\\) is hard in the worst case.<li>\\(\\mathrm{CZK}\\) is hard in the worst case and the problem \\(\\mathrm{GapMCSP}\\) of approximating circuit complexity is in \\(\\mathrm{CZK}\\)</li></ol>This is a joint work with Mikito Nanashima.");
	abstracts.set("simona","In the past few years, there has been an increased interest in hard equivalence problems, especially with NIST's announcement of a fourth round for new designs of digital signatures. On a high level, such a problem can be defined as follows: Given two algebraic objects, find - if any - an equivalence that maps one object into the other. Several instantiations have been considered for cryptographic purposes, for example - Isomorphism of polynomials (Pattarin '96), Code equivalence (Biasse et al. '20), Matrix Code equivalence (Chou et al. '22), Alternating trilinear form equivalence (Tang et al.'22), Lattice isomorphism (Ducas & van Woerden '22). All of these problems are believed to be hard even for quantum adversaries. Conveniently, they can generically be used to build a Sigma protocol and further a post-quantum secure signature using the Fiat-Shamir transform.<br><br>In this talk I will present recent theoretical and practical results on the hardness of some of the above problems. In particuar I will give reductions between the first four problems which show that the Matrix Code equivalence problem (MCE) is at least as difficult as the others. I will also talk about recent non-trivial algorithms for MCE and how they apply to the other problems. Interestingly, these algorithms show that the hardness of these problems has been seriously overestimated in the past.");
	abstracts.set("omri","Quantum computers are expected to revolutionize our ability to process information. The advancement from classical to quantum computing is a product of our advancement from classical to quantum physics -- the more our understanding of the universe grows, so does our ability to use it for computation. A natural question that arises is, what will physics allow in the future? Can more advanced theories of physics increase our computational power, beyond quantum computing?<br><br>An active field of research in physics studies theoretical phenomena outside the scope of explainable quantum mechanics, that form when attempting to combine Quantum Mechanics (QM) with General Relativity (GR) into a unified theory of Quantum Gravity (QG). QG is known to present the possibility of a quantum superposition of causal structure and event orderings. In the literature of quantum information theory, this translates to a superposition of unitary evolution orders.<br><br>In this work we show a first example of a natural computational model based on QG, that provides an exponential speedup over standard quantum computation (under standard hardness assumptions). We define a model and complexity measure for a quantum computer that has the ability to generate a superposition of unitary evolution orders, and show that such computer is able to solve in polynomial time two of the fundamental problems in computer science: The Graph Isomorphism Problem and the Gap Closest Vector Problem, with gap \\(O(n^2)\\) . These problems are believed by experts to be hard to solve for a regular quantum computer. Interestingly, our model does not seem overpowered, and we found no obvious way to solve entire complexity classes that are considered hard in computer science, like the classes <b>NP</b> and <b>SZK</b>.");
	abstracts.set("giulio","");
	abstracts.set("yuval","The question of minimizing different complexity measures of cryptographic primitives is of both theoretical and practical interest. The talk will survey this line of work, discuss connections with error-correcting codes and hardness of learning, and point out some remaining challenges.");
	abstracts.set("afonso","");
	abstracts.set("prashant","I will survey the state of our understanding of the average-case complexity of problems like <i>k</i>-SUM, Subset Sum, OV, <i>k</i>-Clique etc., many of which have recently been the subject of substantial research in the field of fine-grained complexity and beyond.");
	abstracts.set("vinod","I will describe two results at the interface of statistics and machine learning, and cryptography.<br><br>First, in the increasingly common setting where the training of models is outsourced, I will describe a method whereby a malicious trainer can use cryptography to insert an <i>undetectable</i> backdoor in a classifier. Using a secret key, the trainer can then slightly alter inputs to create large deviations in the model output. Without the secret key, the existence of the backdoor is hidden. This result relies on the recently formulated hardness of the continuous learning with errors (CLWE) problem.<br><br>Second, I will show that CLWE is as hard as the widely studied learning with errors (LWE) problem using techniques from leakage-resilient cryptography. In turn, I will use this to show the nearly optimal hardness of the long-studied Gaussian mixture learning problem.<br><br>Based on joint works with Shafi Goldwasser, Michael P. Kim and Or Zamir; and with Aparna Gupte and Neekon Vafa.");
	abstracts.set("tim","I will present new results on when low coordinate degree functions (LCDF)-linear combinations of functions depending on small subsets of entries of a vector can hypothesis test between high-dimensional probability measures. These functions are a generalization of the class of low degree polynomials (LDP) widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and average-case optimization. Though they are a more powerful class of algorithms, LCDF actually admit a much more general theoretical analysis than LDP. I will present two case studies on computational hardness by way of illustration.<br><br>First, I will show that &ldquo;channel universality&rdquo; holds for the success of LCDF in testing for the presence of random signals through noisy channels: the efficacy of LCDF depends on the channel only through a single scalar parameter for a class of channels including nearly arbitrary i.i.d. additive noise and nearly arbitrary exponential families. As a concrete application, I will show that the well-known computationally hard regimes and statistical-to-computational gaps in the spiked matrix and tensor models under additive Gaussian noise are actually universal phenomena, occurring under a wide range of observation models.<br><br>Second, I will give an analysis of testing categorical signals and observations, covering for instance problems over finite groups and stochastic block models observed through different types of interactions between network nodes. LDP usually are not even defined for such problems - what is a polynomial of an element of an abstract group or of &ldquo;red, green, or blue&rdquo;?-but LCDF are easily made sense of in such settings. I will give a unified treatment of a broad class of such problems and discuss examples including community detection and group synchronization.");
	abstracts.set("pravesh","In this talk, we will present a new approach for approximating large independent sets when the input graph is a one-sided expander—that is, the uniform random walk matrix of the graph has a second eigenvalue bounded away from 1. Specifically, we show a polynomial time algorithm that can find linear size-independent sets in one-sided expanders that are almost \\(3\\)-colorable or promised to contain a \\(0.4999n\\) size-independent set.<br><br>All prior algorithms that beat the worst-case guarantees for the problem rely on bottom eigenspace enumeration techniques building Alon and Kahale's classical work and need two-sided expansion (i.e., bounded number of negative eigenvalues of magnitude close to 1). Such techniques must provably fail in our setting because in contrast to bottom eigenspace enumeration that naturally extends to \\(k\\)-colorable graphs for any fixed constant \\(k\\).  We observe that finding linear size independent sets in almost \\(4\\)-colorable graphs (as opposed to almost \\(3\\)-colorable graphs in our algorithmic result above) is NP-hard, assuming the Unique Games Conjecture.<br><br>Our algorithms rely on a new framework for rounding sum-of-squares relaxations of the independent set problem based on a new clustering property of large independent sets in expanders —in any graph that satisfies one-sided spectral expansion (in fact, a weaker vertex expansion property suffices), there is a small list of independent sets such that every large independent sets correlates better than a random set of similar size with at least one member of the list. This is reminiscent of similar clustering properties of solutions space of random optimization problems. Our applications, however, are on deterministic expander graphs, and our algorithmic guarantees need a low-degree sum-of-squares formalization of the clustering property.<br><br>Based on joint work with Mitali Bafna and Tim Hsieh.");
	abstracts.set("chris","Low-degree algorithms are algorithms whose output is a low-degree polynomial function of the input. In recent years, thinking about algorithms as polynomials has provided a host of new perspectives and analysis techniques. First, I will give some background on this approach. Then I will explain a new application of this technique to a class of first-order iterative algorithms which includes belief propagation and Approximate Message Passing (AMP) and many forms of gradient descent. We identify a fundamental property of these algorithms: in the limit \\(N \\rightarrow \\infty\\), we can disregard all of the terms except for the 'tree-shaped terms'. The tree approximation property mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the cavity method by implementing some heuristic physics-based derivations into rigorous proofs. This is joint work with Lucas Pesenti.");
	abstracts.set("miranda","We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.<br><br>We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either \\(2^{O(\\sqrt{n})}\\)-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density. <br><br>As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.<br><br>Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.<br><br>This is joint work with Sam Gunn.");
	abstracts.set("eren","Optimization problems with random objective functions are central in computer science, probability, and modern data science. Despite their ubiquity, finding efficient algorithms for solving these problems remains a major challenge. Interestingly, many random optimization problems share a common feature, dubbed as a statistical-computational gap: while the optimal value can be pinpointed non-constructively (through, e.g., probabilistic/information-theoretic tools), all known polynomial-time algorithms find strictly sub-optimal solutions. That is, an optimal solution can only be found through brute force search which is computationally expensive. ");
	abstracts.set("andrej","The search and decision versions of many planted statistical problems exhibit the same conjectured sample complexity threshold: Soon after a planted instance can be efficiently distinguished from a random one with sufficient advantage, its  solution can be recovered. <br><br>This phenomenon suggests the existence of efficient reductions that implement a solver for planted instances using oracle access to a distinguisher below the computational threshold, indicating the equivalence of hardness for search and pseudorandomness. I will talk about old and new “hardness versus randomness” reductions of this type for dense and sparse noisy random linear equations in modular arithmetic.");
	abstracts.set("ilias_z","");
	abstracts.set("ilias_d","Non-Gaussian component analysis (NGCA) is the following statistical task: Given sample access to a high-dimensional distribution that is non-Gaussian in a hidden direction and a standard multivariate Gaussian in the orthogonal complement, approximate the hidden direction. In a 2017 paper with Kane and Stewart, we established tight lower bounds for this problem in the Statistical Query (SQ) model and derived similar implications for several, seemingly unrelated, statistical tasks. Since then, a number of works have leveraged this framework to obtain tight information-computation tradeoffs for a range of learning problems. In this work, we will survey these developments and discuss opportunities for future work.");
	abstracts.set("nobutaka","In the planted clique problem, given an Erd&#337;s-R&#233;nyi random graph with a \\(k\\)-clique planted randomly, we are asked to find a \\(k\\)-clique of it. It is widely known that this problem exhibits a computational-statistical gap: By the exhaustive search, we can solve this problem if \\(k \\gg \\log n\\), while we are not aware of any efficient (polynomial-time) algorithm for this problem (for \\(\\log n \\ll k \\ll \\sqrt{n}\\)) for decades. This raises the famous <i>Planted Clique Conjecture</i>, which asserts that any polynomial-time algorithm fails to solve the planted clique problem on a certain fraction of inputs. This conjecture serves as a standard hardness hypothesis in the literature of high-dimensional statistics, property testing, one-way functions, game theory, and so on. However, there are many formulations of this problem regarding the success probability (i.e., the fraction of inputs on which we can find a \\(k\\)-clique in polynomial time). What proportion of inputs are solvable in polynomial time?<br><br>We show that the Planted Clique Conjecture is robust with respect to the success probability. Specifically, we show how to transform a polynomial-time algorithm that finds the planted clique on a 1/poly(n)-fraction of inputs into a strong algorithm that finds the planted clique on a \\((1-\\exp(-n^c))\\)-fraction of inputs. As a consequence, we reveal the first detection-recovery gap for the planted clique problem with respect to the success probability.<br><br>Based on a joint work with Shuichi Hirahara.");
	abstracts.set("damiano","We study the following broad question about cryptographic primitives: is it possible to achieve security against an arbitrary \\(\\text{poly}(n)\\)-time adversary with \\(O(\\log n)\\)-size messages? It is common knowledge that the answer is ''no'' unless information-theoretic security is possible. In this work, we revisit this question by considering the setting of cryptography with public information and computational security.<br><br>We obtain the following  results, assuming variants of well-studied intractability assumptions:<ol><li>A private simultaneous messages (PSM) protocol for every \\(f:[n] \\times [n] \\rightarrow \\{0,1\\}\\) requiring \\((1+\\epsilon) \\log n\\)-bit messages for most functions and \\((2+\\epsilon) \\log n\\)-bit messages for the remaining ones. We apply this towards non-interactive secure 3-party computation with similar message size in the preprocessing model, improving over previous 2-round protocols.</li><li>A secret-sharing scheme for any ''forbidden-graph'' access structure on \\(n\\) nodes with \\(O(\\log n)\\) share size.</li><li>On the negative side, we show that computational threshold secret-sharing schemes with public information require share size \\(\\Omega(\\log \\log n)\\). For arbitrary access structures, we show that computational security does not help with 1-bit shares.</li></ol>The above positive results guarantee that any adversary of size \\(n^{o(\\log n)}\\) achieves an \\(n^{-\\Omega(1)}\\) distinguishing advantage. We show how to make the advantage negligible by slightly increasing the asymptotic message size, still improving over all known constructions.<br><br>The security of our constructions is based on the conjectured hardness of variants of the planted clique problem, which was extensively studied in the algorithms, statistical inference, and complexity theory communities. Our work provides the first applications of such assumptions improving the efficiency of mainstream cryptographic primitives, gives evidence for the necessity of such assumptions, and suggests new questions in this domain that may be of independent interest.");
}
